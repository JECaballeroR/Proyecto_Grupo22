{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea4927d3",
   "metadata": {},
   "source": [
    "<div >\n",
    "<img src = \"figs/dsa_banner.png\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e55931d",
   "metadata": {},
   "source": [
    "# Preprocesamiento\n",
    "\n",
    "### Integrantes:\n",
    "    - Catalina García García\n",
    "    - Camilo Alejandro Grande Sánchez\n",
    "    - Jesús Alberto Parada Pérez\n",
    "    - Jorge Esteban Caballero Rodríguez\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a421bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4800453b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b469c8b",
   "metadata": {},
   "source": [
    "## Carga de datos\n",
    "\n",
    "Se cargan los datos de las fuentes dadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45bb0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_datos(filename, nombre_base):\n",
    "    data = pd.read_excel(f\"data/{filename}\")\n",
    "    return data\n",
    "\n",
    "data = cargar_datos(\"Base perfilación de competencias_310823.xlsx\", \"Base de datos:\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c8458b",
   "metadata": {},
   "source": [
    "##  Eliminar faltantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc580e4",
   "metadata": {},
   "source": [
    "Se procede a eliminar los missing en la variable regional en caso de presentarce, ya que para la fundación Future Education es un requisito saber con certeza la regional a la que pertenece un docente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ac78d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Eliminar missing\n",
    "data.dropna(inplace=True, subset = [\"regional\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca38cb67-878c-4d34-bd29-2ad131ee3bfa",
   "metadata": {},
   "source": [
    "### Imputación univariada\n",
    "\n",
    "Se procede a corregir errores comunes en los dominios: \n",
    "- departamento: el dominio NINGUNO no es un valor valido y se debe imputar o eliminar.\n",
    "- ubicacioninstitucion: la ubicación ninguna no es valida, debe ser urbana o rural\n",
    "- edades: todos son docentes, maestros o agentes educativos, por lo que todos son mayores de edad; las edades 0 a 6, 7 a 14 y 15 a 17, no son validas, se debe imputar o eliminar.\n",
    "- sexoinscrito: la opción Mujer no es valida, se debe reemplazar por mujer.\n",
    "\n",
    "Esto se realizará con una imputación univariada con estrategia de la más frecuente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3615327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazar los valores incorrectos con NaN\n",
    "data[\"departamento\"] = data[\"departamento\"].replace(\"NINGUNO\", np.nan)\n",
    "data[\"ubicacioninstitucion\"] = data[\"ubicacioninstitucion\"].replace(\"ninguna\", np.nan)\n",
    "data[\"edades\"] = data[\"edades\"].replace(\n",
    "    [\"Entre 0 y 6 años\", \"Entre 7 y 14 años\", \"Entre 15 y 17 años\"], np.nan\n",
    ")\n",
    "# Reemplazar los valores Mujer con mujer\n",
    "data[\"sexoinscrito\"] = data[\"sexoinscrito\"].replace(\"Mujer\", \"mujer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5955faf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imputar con la moda\n",
    "estrategia=\"most_frequent\"\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy=estrategia)\n",
    "imp.fit(data)\n",
    "# Imputar los valores faltantes en el DataFrame\n",
    "data_imputado = pd.DataFrame(imp.transform(data), columns=data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf96a08-a8db-4ada-96c3-3516885a3b16",
   "metadata": {},
   "source": [
    "## Mapeo Respuestas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50432a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapear las respuestas a los valores numéricos\n",
    "mapeo_respuestas = {\n",
    "    \"Nunca\": 1,\n",
    "    \"A veces\": 2,\n",
    "    \"Constantemente\": 3,\n",
    "    \"Siempre\": 4\n",
    "}\n",
    "\n",
    "# Variables p1 a p20\n",
    "variables_p = ['p1', 'p2', 'p3', 'p4', #Creatividad e innovación\n",
    "               'p5', 'p6', 'p7', 'p8', # Resolucion de problemas\n",
    "               'p9', 'p10', 'p11', 'p12', # Pensamiento critico\n",
    "               'p13', 'p14', 'p15', 'p16', # Trabajo colaborativo\n",
    "               'p17', 'p18', 'p19', 'p20'] # Comunicación\n",
    "\n",
    "# Aplicar el mapeo a las columnas correspondientes\n",
    "for variable in variables_p:\n",
    "    data_imputado[variable] = data_imputado[variable].replace(mapeo_respuestas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f198b8-cbf5-4d6b-bada-8ec64a114f1f",
   "metadata": {},
   "source": [
    "Se hace la codificación y se encuentra que el instrumento tiene preguntas inversas/negativas donde la opción `Nunca` es **Siempre**, `A veces` es **Constantemente**, `Constantemente` es **A veces**, y `Siempre` es **Nunca**.\n",
    "\n",
    "Se procede a invertir estas preguntas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f14dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables que deben invertirse\n",
    "variables_invertir = [\"p2\", \"p4\", \"p7\", \"p8\", \"p11\", \"p14\", \"p16\", \"p18\", \"p20\"]\n",
    "\n",
    "# Invertir las respuestas en las columnas seleccionadas\n",
    "for variable in variables_invertir:\n",
    "    data_imputado[variable] = 5 - data_imputado[variable]  # esto \"invierte\" los alores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c35878b-7970-421a-b407-5f1a263d09ba",
   "metadata": {},
   "source": [
    "## Eliminar registros que no tienen varianza en las respuestas\n",
    "\n",
    "Para todas las preguntas tienen la misma respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efda5721-a34d-45d6-9087-570f3ba917b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecciona las columnas que contienen las respuestas de los docentes (p1 a p20)\n",
    "respuestas = data_imputado.iloc[:, 13:33]\n",
    "\n",
    "# Identifica respuestas constantes por docente\n",
    "respuestas_constantes = respuestas.apply(pd.Series.nunique, axis=1) == 1\n",
    "\n",
    "# Identifica patrones en zigzag por docente\n",
    "patron_zigzag = ((respuestas.diff(axis=1).abs() == 1) & (respuestas.diff(axis=1).notnull())).all(axis=1)\n",
    "\n",
    "# Marca 'Constante' en la columna 'Patron_Respuesta' si todas las respuestas son iguales\n",
    "data_imputado.loc[respuestas_constantes, 'Patron_Respuesta'] = 'Constante'\n",
    "\n",
    "# Marca 'Zigzag' en la columna 'Patron_Respuesta' si hay un patrón de zigzag en las respuestas\n",
    "data_imputado.loc[patron_zigzag, 'Patron_Respuesta'] = 'Zigzag'\n",
    "\n",
    "#Filtrar el dataframe para eliminar registros sin varianza\n",
    "data_imputado = data_imputado[data_imputado['Patron_Respuesta'] != 'Constante']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26507e12-5ccd-4a63-8593-1030e6d22c28",
   "metadata": {},
   "source": [
    "## Validar que un mismo docente no tenga mas de una prueba en menos de 4 meses que dura la intervención minima de un proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2b3608-1238-4fff-ab97-03753312de80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Asegúrate de que 'Marca temporal' sea del tipo datetime, si no lo es, conviértela\n",
    "data_imputado['Marca temporal'] = pd.to_datetime(data_imputado['Marca temporal'])\n",
    "\n",
    "# Ordenar el DataFrame por 'ID' y 'Marca temporal'\n",
    "data_imputado = data_imputado.sort_values(by=['ID', 'Marca temporal'])\n",
    "\n",
    "# Identificar pruebas realizadas por el mismo docente con menos de 6 meses de diferencia\n",
    "data_imputado['Diferencia_meses'] = data_imputado.groupby('ID')['Marca temporal'].diff().dt.days // 30\n",
    "\n",
    "# Encuentra los índices de los registros que deben ser excluidos\n",
    "indices_a_eliminar = data_imputado[(data_imputado['Diferencia_meses'].lt(4) & data_imputado['Diferencia_meses'].ge(0))].index\n",
    "\n",
    "# Elimina los registros identificados del DataFrame original\n",
    "data_imputado = data_imputado.drop(indices_a_eliminar)\n",
    "\n",
    "# Elimina la columna creada\n",
    "data_imputado = data_imputado.drop(columns='Diferencia_meses')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81bb670-4ad0-41c1-9196-9c624f82b30a",
   "metadata": {},
   "source": [
    "## Analisis Factorial\n",
    "\n",
    "Antes de implementar PCA, vamos a realizar un analisis factorial para identificar posibles patrones subyacentes en los datos que expliquen su variabilidad. Para esto primero utilizaremos el test de esfericidad de Bartlett para determinar si existe una estructura de correlación entre las variables observadas que pueda ser explotada mediante técnicas como el análisis factorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef0ad10-dabc-44ea-99c9-bda80293302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando un analisis factorial, aqui utilizamos una roration ortogonal porque las variables muestran en la matriz de correlación que son independientes\n",
    "fa = FactorAnalyzer(rotation='varimax')\n",
    "fa.fit(data_imputado.iloc[:, 13:33])\n",
    "\n",
    "# Check Eigenvalues\n",
    "ev, v = fa.get_eigenvalues()\n",
    "\n",
    "# Contar la cantidad de eigenvalues mayores a 1\n",
    "factores = sum(ev > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31d5166-fced-449a-8574-ef01ee76f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar analisis factorial con tres factores (eigenvalues mayor a 1)\n",
    "fa = FactorAnalyzer(n_factors=factores, rotation='varimax')\n",
    "fa.fit(data_imputado.iloc[:, 13:33])\n",
    "\n",
    "# Creando un dataframe con los pesos\n",
    "loadings_dataframe = pd.DataFrame(fa.loadings_, index=variables_p)\n",
    "\n",
    "# Función para asignar cada pregunta al factor con el loading más alto\n",
    "def asignar_a_factor(loadings, variables):\n",
    "    assigned_factors = {}\n",
    "    for variable in variables:\n",
    "        abs_loadings = np.abs(loadings.loc[variable])\n",
    "        max_loading_factor = abs_loadings.idxmax()\n",
    "        assigned_factors[variable] = max_loading_factor\n",
    "    return assigned_factors\n",
    "\n",
    "# Asignar cada pregunta al factor correspondiente\n",
    "assigned_factors = asignar_a_factor(loadings_dataframe, variables_p)\n",
    "\n",
    "# Crear listas de preguntas para cada factor basadas en las asignaciones\n",
    "preguntas_factor = {factor: [pregunta for pregunta, asignado in assigned_factors.items() if asignado == factor] for factor in range(3)}\n",
    "\n",
    "# Obtener loadings por factor según las asignaciones\n",
    "loadings_por_factor = {}\n",
    "for factor, preguntas in preguntas_factor.items():\n",
    "    loadings_por_factor[factor + 1] = loadings_dataframe.loc[preguntas, factor]\n",
    "\n",
    "# Iterar sobre los factores y sus loadings asociados\n",
    "for factor, loadings in loadings_por_factor.items():\n",
    "    # Multiplicar las respuestas correspondientes a cada factor por los loadings para obtener los puntajes por factor\n",
    "    name_pregunta = loadings.index.tolist()  # Obtener las preguntas asociadas al factor\n",
    "    data_imputado[f'Puntaje_Factor_{factor}'] = data_imputado[name_pregunta].dot(loadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86d89be-665c-4904-b400-b213e42281dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los pesos de los factores\n",
    "pesos_factores = fa.get_factor_variance()[0]\n",
    "\n",
    "# Obtener las columnas de puntajes por factor en el DataFrame\n",
    "columnas_puntajes_factor = [col for col in data_imputado.columns if col.startswith('Puntaje_Factor_')]\n",
    "\n",
    "# Multiplicar los puntajes de cada factor por sus respectivos pesos\n",
    "data_imputado['Puntaje_General'] = np.sum(data_imputado[columnas_puntajes_factor] * pesos_factores[:len(columnas_puntajes_factor)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc685148-a3f8-4fbf-89c4-e333b63a1cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estandarizar los puntajes para que queden todos en una misma medida\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Seleccionar las columnas de puntajes\n",
    "puntajes = data_imputado[['Puntaje_Factor_1', 'Puntaje_Factor_2', 'Puntaje_Factor_3', 'Puntaje_General']]\n",
    "\n",
    "# Crear un objeto StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Estándarizar los puntajes\n",
    "puntajes_estandarizados = scaler.fit_transform(puntajes)\n",
    "\n",
    "# Agregar columnas de puntajes estandarizados al DataFrame\n",
    "data_imputado['Puntaje_Factor_1_Estandarizado'] = puntajes_estandarizados[:, 0]\n",
    "data_imputado['Puntaje_Factor_2_Estandarizado'] = puntajes_estandarizados[:, 1]\n",
    "data_imputado['Puntaje_Factor_3_Estandarizado'] = puntajes_estandarizados[:, 2]\n",
    "data_imputado['Puntaje_General_Estandarizado'] = puntajes_estandarizados[:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def06438-f640-4fe1-9873-cdb95583df91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los intervalos para niveles Bajo y Alto\n",
    "intervalos = [-1e10, 0, 1e10]  # Cualquier valor menor a cero es 'Bajo', igual o mayor a cero es 'Alto'\n",
    "etiquetas = ['Bajo', 'Alto']\n",
    "\n",
    "# Asignar niveles (Bajo y Alto) usando pd.cut para cada puntaje estandarizado\n",
    "data_imputado[\"Nivel_Puntaje_Factor_1\"] = pd.cut(data_imputado[\"Puntaje_Factor_1_Estandarizado\"], bins=intervalos, labels=etiquetas)\n",
    "data_imputado[\"Nivel_Puntaje_Factor_2\"] = pd.cut(data_imputado[\"Puntaje_Factor_2_Estandarizado\"], bins=intervalos, labels=etiquetas)\n",
    "data_imputado[\"Nivel_Puntaje_Factor_3\"] = pd.cut(data_imputado[\"Puntaje_Factor_3_Estandarizado\"], bins=intervalos, labels=etiquetas)\n",
    "data_imputado[\"Nivel_Puntaje_General\"] = pd.cut(data_imputado[\"Puntaje_General_Estandarizado\"], bins=intervalos, labels=etiquetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151852bc-5381-4099-9d87-9923650bfb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputado.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e14b5c1-63d2-4779-bb38-f685d35f3f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputado = data_imputado.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9796ce",
   "metadata": {},
   "source": [
    "## Selección de Componentes - PCA\n",
    "\n",
    "A partir del análisis realizado anteriormente se decidió dejar únicamente las preguntas que afectan las 12 primeras componentes las cuales son:\n",
    "    ['p7', 'p12', 'p5', 'p15', 'p9', 'p2', 'p17', 'p18', 'p20', 'p6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79fd0ac-74d8-48b1-bdf4-9ed0ab364b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Crea el objeto PCA\n",
    "pca_model = PCA(n_components=20)\n",
    "\n",
    "# Ajusta el modelo PCA a tus datos\n",
    "modelo_pca = pca_model.fit_transform(data_imputado.iloc[:, 14:34]) #tomar las 20 preguntas\n",
    "\n",
    "# Explora la varianza explicada para determinar la cantidad óptima de componentes\n",
    "explained_variance = pca_model.explained_variance_ratio_\n",
    "cumulative_explained_variance = explained_variance.cumsum()\n",
    "\n",
    "# Encuentra el número óptimo de componentes basado en la varianza explicada acumulativa\n",
    "optimal_n_components = (cumulative_explained_variance <= 0.8).sum()+1   # Por ejemplo, puedes elegir 95% de varianza explicada\n",
    "\n",
    "# Ajusta el modelo PCA con la cantidad óptima de componentes\n",
    "pca_model = PCA(n_components=optimal_n_components)\n",
    "modelo_pca = pca_model.fit_transform(data_imputado.iloc[:, 14:34])\n",
    "\n",
    "# Calcula la varianza explicada acumulativa\n",
    "explained_variance = pca_model.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf78f2af-5725-48f4-bf54-c742e92d7e7a",
   "metadata": {},
   "source": [
    "## Modelo no supervisado de clustering con PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df18bc08-c205-4cb1-a7e8-b6ca0b8f9cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# Aplicar K-means para agrupar los datos\n",
    "kmeans = KMeans(n_clusters=2)  # Se eligen 2 clusters (Bajo y Alto)\n",
    "kmeans.fit(modelo_pca)\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Crear un DataFrame con los resultados del clustering y el puntaje\n",
    "cluster_df = pd.DataFrame(cluster_labels, columns=['Cluster'])\n",
    "cluster_df['Puntaje'] = data_imputado['Nivel_Puntaje_General']\n",
    "\n",
    "# Calcular la inercia de los clusters\n",
    "inercia = kmeans.inertia_\n",
    "print(f'Inercia de los clusters: {inercia}')\n",
    "\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "# Calcular el índice de Davies-Bouldin\n",
    "davies_bouldin = davies_bouldin_score(modelo_pca, cluster_labels)\n",
    "print(f'Índice de Davies-Bouldin: {davies_bouldin}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d88bab6-4772-4c2e-8538-4e77ee0dacc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def heatmap_count(df, var1, var2):\n",
    "    \"\"\"Generates a Count heatmap of any two columns of a data frame\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        A DataFrame that has column features\n",
    "    var1 : str\n",
    "        The name of a column in the DataFrame\n",
    "    var2 : str\n",
    "        The name of a column in the DataFrame. Different from var1\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    plot : seaborn.heatmap\n",
    "        A heatmap of the 2 variables grouped by count, with preset configuration\n",
    "    \"\"\"\n",
    "\n",
    "    heat_df = pd.DataFrame(df.groupby([var1, var2]).size()).reset_index()\n",
    "    heat_df = heat_df.pivot(columns=var1, index=var2, values=0)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(heat_df, cmap=\"Blues\", linewidths=0.5, annot=True, fmt='g')\n",
    "    plt.xlabel(var1)\n",
    "    plt.ylabel(var2)\n",
    "    plt.title(f'Heatmap of {var1} vs {var2}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0aea1e-b2db-41d9-aa15-918d4ca11b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar el heatmap de Cluster vs Nivel_Puntaje_General\n",
    "heatmap_count(cluster_df, 'Cluster', 'Puntaje')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77781c38-b410-4ace-b136-5900826c20a7",
   "metadata": {},
   "source": [
    "## Modelo Random Forest Regresor utilizando la variable a predecir que se obtuvo del analisis factorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5524ca14-b8b3-4a27-a4bc-cb71b73e4f62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# defina el servidor para llevar el registro de modelos y artefactos\n",
    "mlflow.set_tracking_uri('http://localhost:8080')\n",
    "# registre el experimento\n",
    "experiment = mlflow.set_experiment(\"Random_Forest_Model_Experiments\")\n",
    "# Variables predictoras\n",
    "col_select=['p7', 'p12', 'p5', 'p15', 'p9', 'p2', 'p17', 'p18', 'p20', 'p6']\n",
    "# X = data_imputado.iloc[:, 14:34]\n",
    "X = data_imputado[col_select]\n",
    "# Variable a predecir\n",
    "y = data_imputado[\"Puntaje_General_Estandarizado\"]\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba (80% entrenamiento, 20% prueba)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Definir la grilla de hiperparámetros a probar para el RandomForestRegressor\n",
    "\n",
    "n_estimators= [100, 200, 300, 400, 500]\n",
    "max_depths= [None, 5, 10, 15]\n",
    "min_samples_splits= [2, 5, 10]\n",
    "min_samples_leafs= [1, 2, 4]\n",
    "\n",
    "for n_estimator in n_estimators:\n",
    "    for max_depth in max_depths:\n",
    "        for min_samples_split in min_samples_splits:\n",
    "            for min_samples_leaf in min_samples_leafs:\n",
    "                with mlflow.start_run(experiment_id=experiment.experiment_id,\n",
    "                                     run_name =f\"random-forest-model-num_trees{n_estimator}-maxdepth{max_depth}-ssplit{min_samples_split}-sleaf{min_samples_leaf}\"):\n",
    "\n",
    "            \n",
    "                    # Crear el modelo de RandomForestRegressor\n",
    "                    rf = RandomForestRegressor(n_estimators = n_estimator,\n",
    "                                               max_depth = max_depth,\n",
    "                                              min_samples_split  = min_samples_split,\n",
    "                                              min_samples_leaf = min_samples_leaf)\n",
    "                    rf.fit(X_train, y_train)\n",
    "\n",
    "                    predictions = rf.predict(X_test)\n",
    "\n",
    "\n",
    "                        # Registre los parámetros\n",
    "                    mlflow.log_param(\"num_trees\", n_estimator)\n",
    "                    mlflow.log_param(\"maxdepth\", max_depth)\n",
    "                    mlflow.log_param(\"min_samples_split\", min_samples_split)\n",
    "                    mlflow.log_param(\"min_samples_leaf\", min_samples_leaf)\n",
    "\n",
    "                    mlflow.sklearn.log_model(rf, f\"random-forest-model-num_trees{n_estimator}-maxdepth{max_depth}-ssplit{min_samples_split}-sleaf{min_samples_leaf}\")\n",
    "\n",
    "                    mse = mean_squared_error(y_test, predictions)\n",
    "                    mlflow.log_metric(\"mse\", mse)\n",
    "                    print(mse)\n",
    "                    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b26628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d941e4d9-e418-4059-8fff-23f8a972b718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cdb735-939b-4668-bcae-a07862b78ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4007b969-0b52-4940-b638-f9b7335ff62d",
   "metadata": {},
   "source": [
    "#componentes a eliminar:\n",
    "coldrops = ['p1', 'p3', 'p4', 'p8', 'p10', 'p11', 'p13', 'p14', 'p16', 'p19']\n",
    "\n",
    "def select_comp(data):\n",
    "    data_sc = data.drop(coldrops,axis=1)\n",
    "    return data_sc\n",
    "\n",
    "data_sc = select_comp(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a0fc42",
   "metadata": {},
   "source": [
    "## Separar y almacenar datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266f3f0c",
   "metadata": {},
   "source": [
    "Se realiza el almacenamiento de los datos y se generan bases con solo los socioeconómicos para visualización y las preguntas para los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b1dfdc-a7ee-47be-b1ce-e7b237a60f95",
   "metadata": {},
   "source": [
    "col_select=['p7', 'p12', 'p5', 'p15', 'p9', 'p2', 'p17', 'p18', 'p20', 'p6']\n",
    "col_select2=['ID','p7', 'p12', 'p5', 'p15', 'p9', 'p2', 'p17', 'p18', 'p20', 'p6']\n",
    "data_SE = data_imputado.drop(col_select,axis=1)\n",
    "data_p = data_imputado[col_select2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9379dee2-b86d-43be-82fc-d6dcf0e2b9d2",
   "metadata": {},
   "source": [
    "data_imputado.to_excel('data/BD_G22.xlsx')\n",
    "data_p.to_excel('data/BD_G22_P.xlsx')\n",
    "data_SE.to_excel('data/BD_G22_SE.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
